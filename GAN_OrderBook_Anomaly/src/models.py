"""
GAN model architectures for order book data.

This module implements GRU-based Generator and Discriminator networks
for learning order book dynamics and detecting anomalies.
"""

import torch
import torch.nn as nn
from typing import Tuple, Optional


class Generator(nn.Module):
    """
    GRU-based Generator network for synthetic order book generation.

    The generator transforms random noise into realistic order book sequences.
    It uses a series of GRU layers to capture temporal dependencies, with
    interleaved linear transformations for feature mixing.

    Architecture:
        - 4 GRU layers with varying hidden dimensions
        - LeakyReLU activations to prevent dead neurons
        - Input: (batch, seq_len, 20) noise
        - Output: (batch, seq_len, 20) synthetic order book

    Example:
        >>> generator = Generator()
        >>> noise = torch.randn(32, 265, 20)  # batch=32, minutes=265
        >>> synthetic = generator(noise)
        >>> synthetic.shape
        torch.Size([32, 265, 20])
    """

    def __init__(self, input_dim: int = 20, hidden_dim: int = 40):
        """
        Initialize Generator.

        Args:
            input_dim: Dimension of input features (default: 20 for 5-level LOB)
            hidden_dim: Hidden dimension for GRU layers (default: 40)
        """
        super(Generator, self).__init__()

        # Layer 1: Initial GRU to process input noise
        self.gru1 = nn.GRU(input_dim, hidden_dim, num_layers=1, batch_first=True)
        self.fc1 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.01),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Layer 2: Deeper representation learning
        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layers=1, batch_first=True)
        self.fc2 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.01),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Layer 3: Dimension reduction
        self.gru3 = nn.GRU(hidden_dim, 19, num_layers=1, batch_first=True)
        self.fc3 = nn.Sequential(
            nn.Linear(19, hidden_dim),
            nn.LeakyReLU(0.01),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Layer 4: Final output generation
        self.gru4 = nn.GRU(hidden_dim, input_dim, num_layers=1, batch_first=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the generator.

        Args:
            x: Input noise tensor of shape (batch, seq_len, input_dim)

        Returns:
            Synthetic order book sequence of shape (batch, seq_len, input_dim)
        """
        # GRU layer 1
        y, _ = self.gru1(x)
        z = self.fc1(y)

        # GRU layer 2
        u, _ = self.gru2(z)
        v = self.fc2(u)

        # GRU layer 3
        w, _ = self.gru3(v)
        o = self.fc3(w)

        # GRU layer 4 (output)
        p, _ = self.gru4(o)

        return p


class Discriminator(nn.Module):
    """
    GRU-based Discriminator network for real/fake classification.

    The discriminator learns to distinguish between real order book data
    and synthetic sequences generated by the Generator. After training,
    it can be repurposed for anomaly detection.

    Architecture:
        - 4 GRU layers with dropout for regularization
        - Sigmoid output for probability score
        - Input: (batch, seq_len, 20) order book sequence
        - Output: (batch, 1) probability score (1 = real)

    Example:
        >>> discriminator = Discriminator()
        >>> data = torch.randn(32, 265, 20)
        >>> scores = discriminator(data)
        >>> scores.shape
        torch.Size([32, 1])
    """

    def __init__(self, input_dim: int = 20, hidden_dim: int = 40, dropout: float = 0.15):
        """
        Initialize Discriminator.

        Args:
            input_dim: Dimension of input features (default: 20)
            hidden_dim: Hidden dimension for GRU layers (default: 40)
            dropout: Dropout rate for regularization (default: 0.15)
        """
        super(Discriminator, self).__init__()

        # Layer 1: Initial GRU with 2 stacked layers for capacity
        self.gru1 = nn.GRU(input_dim, hidden_dim, num_layers=2, batch_first=True)
        self.fc1 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.01),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Layer 2
        self.gru2 = nn.GRU(hidden_dim, hidden_dim, num_layers=1, batch_first=True)
        self.fc2 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.01),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Layer 3
        self.gru3 = nn.GRU(hidden_dim, hidden_dim, num_layers=1, batch_first=True)
        self.fc3 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.01),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Layer 4: Final classification
        self.gru4 = nn.GRU(hidden_dim, hidden_dim, num_layers=1, batch_first=True)
        self.fc4 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.01),
            nn.Linear(hidden_dim, 1)  # Single output for real/fake
        )

        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through the discriminator.

        Args:
            x: Order book sequence of shape (batch, seq_len, input_dim)

        Returns:
            Probability score of shape (batch, 1), range [0, 1]
        """
        # GRU layer 1
        y, _ = self.gru1(x)
        z = self.dropout(self.fc1(y))

        # GRU layer 2
        v, _ = self.gru2(z)
        u = self.fc2(v)

        # GRU layer 3
        w, _ = self.gru3(u)
        r = self.fc3(w)

        # GRU layer 4
        s, _ = self.gru4(r)
        t = self.fc4(s)

        # Use last timestep and apply sigmoid
        return torch.sigmoid(t[:, -1, :])


def create_gan(input_dim: int = 20, hidden_dim: int = 40) -> Tuple[Generator, Discriminator]:
    """
    Create Generator and Discriminator pair.

    Args:
        input_dim: Input feature dimension
        hidden_dim: Hidden layer dimension

    Returns:
        Tuple of (Generator, Discriminator)
    """
    generator = Generator(input_dim, hidden_dim)
    discriminator = Discriminator(input_dim, hidden_dim)
    return generator, discriminator


def count_parameters(model: nn.Module) -> int:
    """Count trainable parameters in a model."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def save_models(generator: Generator, discriminator: Discriminator,
                path: str, stock: str) -> None:
    """
    Save trained models to disk.

    Args:
        generator: Trained generator
        discriminator: Trained discriminator
        path: Directory to save models
        stock: Stock ticker for filename
    """
    torch.save(generator, f"{path}/{stock}_generator.pth")
    torch.save(discriminator, f"{path}/{stock}_discriminator.pth")


def load_models(path: str, stock: str) -> Tuple[Generator, Discriminator]:
    """
    Load trained models from disk.

    Args:
        path: Directory containing models
        stock: Stock ticker

    Returns:
        Tuple of (Generator, Discriminator)
    """
    generator = torch.load(f"{path}/{stock}_generator.pth", weights_only=False)
    discriminator = torch.load(f"{path}/{stock}_discriminator.pth", weights_only=False)
    return generator, discriminator
